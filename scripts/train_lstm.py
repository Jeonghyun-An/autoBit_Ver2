# scripts/train_lstm.py
import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from torch import nn
from app.models.lstm_model import LSTMPricePredictor
from app.services.data_loader import fetch_upbit_data
import joblib


# NOTE: LSTM ëª¨ë¸ í•™ìŠµ â†’ ì €ì¥ â†’ ë¶ˆëŸ¬ì˜¤ê¸°

# ë°ì´í„° ë¡œë”©
df = fetch_upbit_data(count=300)
if df is None or len(df) < 60:
    print("ë°ì´í„° ë¶€ì¡±")
    exit()
close = df['close'].values.reshape(-1, 1)

# ì •ê·œí™”
scaler = MinMaxScaler()
scaled = scaler.fit_transform(close)

# ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
def make_sequences(data, seq_len=60):
    x, y = [], []
    for i in range(len(data) - seq_len):
        x.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return np.array(x), np.array(y)

x_data, y_data = make_sequences(scaled)

x_tensor = torch.tensor(x_data).float()
y_tensor = torch.tensor(y_data).float()

# ëª¨ë¸ í•™ìŠµ
model = LSTMPricePredictor()
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

print("ğŸ” LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
epochs = 50
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(x_tensor)
    loss = loss_fn(output, y_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.6f}")

# ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
torch.save({
    "model_state_dict": model.state_dict(),
}, "model_lstm.pt")

joblib.dump(scaler, "scaler_lstm.pkl")

print("âœ… LSTM ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model_lstm.pt")